---
output:
  html_document:
    highlight: haddock
    theme: cerulean
---

![](logo.jpg)

Métodos de Predictivos (Clasificación o Aprendizaje-Supervisado)
========================================================

Dr. Oldemar Rodríguez
--------------------------

# Índices de Calidad del Modelo y funciones auxiliares

```{r st1,comment=NA,warning=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(glue)
library(scales)

equilibrio.variable.predecir <- function(datos, variable.predecir, ylab = "Cantidad de individuos", 
                                        xlab = "", main = paste("Distribución de la variable",variable.predecir), col = NA) {
  gg_color <- function (n) {
     hues <- seq(15, 375, length = n + 1)
     hcl(h = hues, l = 65, c = 100)[1:n]
  }
  if(missing(variable.predecir) | !(variable.predecir %in% colnames(datos))){
    stop("variable.predecir tiene que ser ingresada y ser un nombre de columna", call. = FALSE )
  }
  if(is.character(datos[,variable.predecir]) | is.factor(datos[,variable.predecir])){
    if(length(col) == 0 || is.na(col)){
      col <- gg_color(length(unique(datos[,variable.predecir])))
    }else{
      col <- rep(col,length(unique(datos[,variable.predecir])))
    }
    ggplot(data = datos, mapping = aes_string(x = variable.predecir, fill = variable.predecir)) +
      geom_bar() +
      scale_fill_manual(values = col, name = variable.predecir) +
      labs(x = xlab, y = ylab, title = main) +
      theme_minimal() +
      theme(legend.position = "bottom")
  }else{
    stop("La variable a predecir tienen que ser de tipo factor o character", call. = FALSE )
  }
}

poder.predictivo.numerica <- function(datos, variable.predecir, variable.comparar, ylab = "", 
                                       xlab = "", main = paste("Densidad de la variable", variable.comparar, 'según', variable.predecir), col = NA){
  gg_color <- function (n) {
     hues <- seq(15, 375, length = n + 1)
     hcl(h = hues, l = 65, c = 100)[1:n]
  }
  if(missing(variable.predecir) | !(variable.predecir %in% colnames(datos))){
    stop("variable.predecir tiene que ser ingresada y ser un nombre de columna", call. = FALSE )
  }
  if(missing(variable.comparar) | !(variable.comparar %in% colnames(datos)) | !is.numeric(datos[,variable.comparar])){
    stop("variable.comparar tiene que ser ingresada y ser un nombre de columna numérica", call. = FALSE )
  }
  
  if(is.character(datos[,variable.predecir]) | is.factor(datos[,variable.predecir])){
    if(length(col) == 0 || is.na(col)){
      col <- gg_color(length(unique(datos[,variable.predecir])))
    }else{
      col <- rep(col,length(unique(datos[,variable.predecir])))
    }
    
    ggplot(data = datos, aes_string(variable.comparar, fill = variable.predecir)) +
      geom_density(alpha = .7, color = NA) +
      scale_fill_manual(values = col) +
      labs(title = main , y = ylab, x = xlab ,fill = variable.predecir) +
      theme_minimal() +
      theme(legend.position = 'bottom',
            legend.title = element_blank(),
            text = element_text(size = 15))
    
  }else{
    stop("La variable a predecir tienen que ser de tipo factor o character", call. = FALSE )
  }
}

poder.predictivo.categorica <- function(datos, variable.predecir, variable.comparar, ylab = "", 
                                        xlab = "", main = paste("Densidad de la variable", variable.comparar, 'según', variable.predecir), col = NA) {
  gg_color <- function (n) {
     hues <- seq(15, 375, length = n + 1)
     hcl(h = hues, l = 65, c = 100)[1:n]
  }
  if(missing(variable.predecir) | !(variable.predecir %in% colnames(datos))){
    stop("variable.predecir tiene que ser ingresada y ser un nombre de columna", call. = FALSE )
  }
  if(missing(variable.comparar) | !(variable.comparar %in% colnames(datos)) | 
     !(is.factor(datos[,variable.comparar]) | is.character(datos[,variable.comparar])) ){
    stop("variable.comparar tiene que ser ingresada y ser un nombre de columna categórica", call. = FALSE )
  }
  
  if(is.character(datos[,variable.predecir]) | is.factor(datos[,variable.predecir])){
    if(length(col) == 0 || is.na(col)){
      col <- gg_color(length(unique(datos[,variable.predecir])))
    }else{
      col <- rep(col,length(unique(datos[,variable.predecir])))
    }
    
    datos2 <- datos %>%
      dplyr::group_by_(variable.comparar, variable.predecir) %>%
      dplyr::summarise(count = n())
    
    if(variable.comparar != variable.predecir){
      datos2 <-   datos2 %>% dplyr::group_by_(variable.comparar)
    }
    datos2 <- datos2 %>% dplyr::mutate(prop = round(count/sum(count),4))
  
    ggplot(data = datos2, mapping = aes_string(x = variable.comparar, y = "prop", fill = variable.predecir)) +
      geom_col(position = "fill") +
      geom_text(aes(label = glue("{percent(prop)} ({count})")), position = position_stack(vjust = .5), color = "white") +
      scale_y_continuous(label = percent) +
      labs(y =  xlab, x  = ylab, title = main) +
      scale_fill_manual(values = col, name = variable.predecir) +
      theme(legend.position = "bottom")+
      coord_flip()
    
  }else{
    stop("La variable a predecir tienen que ser de tipo factor o character", call. = FALSE )
  }
}
# Índices para matrices NxN
indices.general <- function(MC) {
  precision.global <- sum(diag(MC))/sum(MC)
  error.global <- 1 - precision.global
  precision.categoria <- diag(MC)/rowSums(MC)
  res <- list(matriz.confusion = MC, precision.global = precision.global, error.global = error.global, 
              precision.categoria = precision.categoria)
  names(res) <- c("Matriz de Confusión", "Precisión Global", "Error Global", 
                  "Precisión por categoría")
  return(res)
}
```


# El método de XGBoosting

## Métodos de Potenciación - eXtreme Gradient Boosting - para 2 clases

### Ejemplo Scoring

```{r boos12,comment=NA}
library(xgboost)
# Leyendo Datos
setwd("~/Google Drive/MDCurso/Datos")
datos<-read.csv("MuestraCredito5000V2.csv",sep = ";",header=T)
tam<-dim(datos)
# Recodifica las variables como categóricas ordinales
datos$IngresoNeto <- factor(datos$IngresoNeto,ordered = TRUE)
datos$CoefCreditoAvaluo <- factor(datos$CoefCreditoAvaluo,ordered = TRUE)
str(datos)
## Vamos a generar al azar una tabla de testing de tamaño 50 y una tabla de aprendizaje de tamaño 100.
n<-tam[1]
muestra <- sample(1:n,floor(n*0.15))
ttesting <- datos[muestra,]
taprendizaje <- datos[-muestra,]
# Recodifica la variable a predecir pues solo trabaja con "1" y "0"
taprendizaje$BuenPagador <- as.numeric(ifelse(taprendizaje$BuenPagador == "Si", "1", "0"))
ttesting$BuenPagador <- as.numeric(ifelse(ttesting$BuenPagador == "Si", "1", "0"))

# Guarda la variable a predecir
valor.variable.predecir <- ttesting$BuenPagador

# Convierte a int de num
taprendizaje[] <- lapply(taprendizaje, as.numeric)
ttesting[] <- lapply(ttesting, as.numeric)

# Las adapta al paquete xgboost 
taprendizaje <- xgb.DMatrix(data = data.matrix(taprendizaje[,-6]),label = data.matrix(taprendizaje$BuenPagador))
ttesting <- xgb.DMatrix(data = data.matrix(ttesting[,-6]),label = data.matrix(ttesting$BuenPagador))

# Parámetros del modelo
parametros <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, 
               gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)
modelo <- xgb.train (params = parametros, data = taprendizaje, nrounds = 79, 
                  watchlist = list(train=taprendizaje, test=ttesting), print_every_n = 10, 
                  early_stop_round = 10, maximize = F , eval_metric = "error")

prediccion <- predict (modelo, ttesting)
head(prediccion)
prediccion <- ifelse (prediccion > 0.5, 1, 0)
MC <- table(prediccion, valor.variable.predecir)
MC
# Índices de Calidad de la predicción
indices.general(MC)

#Importancia de las Variables
variables.importantes <- xgb.importance(feature_names = colnames(taprendizaje), model = modelo)
xgb.plot.importance(importance_matrix = variables.importantes)
```

## Métodos de Potenciación - eXtreme Gradient Boosting - para más de 2 clases

### Ejemplo Iris

```{r boos13,comment=NA}
setwd("~/Google Drive/MDCurso/Datos")
datos<-read.csv("iris.csv",sep = ";",dec='.',header=T)
muestra <- sample(1:150,50)
ttesting <- datos[muestra,]
taprendizaje <- datos[-muestra,]
taprendizaje$tipo <- as.numeric(taprendizaje$tipo)-1
ttesting$tipo <- as.numeric(ttesting$tipo)-1
valor.variable.predecir <- ttesting$tipo

taprendizaje <- xgb.DMatrix(data = data.matrix(taprendizaje[,-5]),label = data.matrix(taprendizaje$tipo))
ttesting <- xgb.DMatrix(data = data.matrix(ttesting[,-5]),label = data.matrix(ttesting$tipo))

# Parámetros del modelo
# OJO objective = "multi:softprob"
parametros <- list(booster = "gbtree", objective = "multi:softprob", eta=0.3, 
                   gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1,num_class = 3)

# OJO eval_metric = "mlogloss"
modelo <- xgb.train(params = parametros, data = taprendizaje, nrounds = 79, 
                     watchlist = list(train=taprendizaje, test=ttesting), print_every_n = 10, 
                     early_stop_round = 10, maximize = F , eval_metric = "mlogloss")

prediccion <- predict(modelo, ttesting)
head(prediccion)
# Convierte la probabilidad en una matriz con tres probabilidades
prediccion <- matrix(prediccion, ncol=3, byrow=TRUE)
head(prediccion)
# Convierte las probabilidades a etiquetas de clase
pred_clase <- max.col(prediccion) - 1
head(pred_clase)

MC <- table(pred_clase, valor.variable.predecir)
MC
# Índices de Calidad de la predicción
indices.general(MC)

#Importancia de las Variables
variables.importantes <- xgb.importance(feature_names = colnames(taprendizaje), model = modelo)
xgb.plot.importance(importance_matrix = variables.importantes)
```
